{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to datakb","text":"<p>datakb is a data knowledge base. Our goal is to write a guide on good data practices for bigger size Enterprises. There is a lot of information about analytics and AI out there, but it is usually either too technical or not detailed enough, we would like to correct that by creating a guide that caters to technical and business professionals and goes into the details of what works and what doesn't.</p>"},{"location":"#principles","title":"Principles","text":"<p>This is an opinionated guide, and we follow strictly the following principles:</p> <ul> <li>Data initiatives must add value. By adding value, we mean that data initiatives should enhance business processes, outcomes, or capabilities. We do not believe in building technology in a vacuum.</li> <li>The TCO (Total Cost of Ownership) increase over time must be considered. Too often, we have seen data science projects promise significant transformations but end up causing substantial increases in IT costs with minimal impact to show for it.</li> <li>Business knowledge is crucial to creating value in data initiatives; close business proximity is essential for effective analytics.</li> <li>Data governance is critical; without it, companies risk developing a complex web of databases, tables, files, and code that becomes increasingly difficult and expensive to maintain over time.</li> <li>Data teams should be organized around clear principles, goals, and rules of engagement. Data organizations should be intentionally built to remove obstacles and support value creation.</li> </ul>"},{"location":"#contacts","title":"Contacts","text":"Area Docs Owner Marts Marcin Socha link Organization Maro Sola link Data Governance Alessio Civitillo link Data Platform Karol Wolski link"},{"location":"data_governance/op_model/","title":"Process","text":""},{"location":"data_governance/roles/","title":"Roles","text":""},{"location":"data_governance/roles/#data-engineer","title":"Data Engineer","text":""},{"location":"data_governance/roles/#job-description","title":"Job Description","text":""},{"location":"data_platform/cicd/","title":"CI/CD","text":"<p>CI/CD process for deploying and managing a data platform is visible in multiple places. Starting from infrastructure, there should be no place for manually created server, network or any other resources, with tools like Terraform or Ansible managing all of it. This part is explained further in Infrastructure as Code page. Second CI/CD layer relates to application part, where all changes within containers running on a cluster should be tested and prepared in an automated way. Final place for CI/CD is within data flows that are going to be executed on orchestrator like Prefect, Apache Airflow or Dagster. This part shouldn't allow manual changes with everything being hosted on repository like GitHub or Gitlab. Here we will focus on typical CI/CD for application part, and also on Data Flow Automation</p>"},{"location":"data_platform/cicd/#application-deployment-automation","title":"Application Deployment Automation","text":"<p>In this chapter, we\u2019ll dive into the technical details of setting up a robust CI/CD pipeline using GitHub workflows, Dockerfiles, and Helm charts for deployments on a Kubernetes cluster. By combining these tools, we can streamline the process of building, testing, and deploying applications efficiently. We\u2019ll walk through how GitHub workflows automate tasks, Dockerfiles enable consistent application environments, and Helm charts facilitate scalable Kubernetes deployments\u2014ultimately creating a seamless pipeline from code commit to production-ready deployments. This setup is essential for maintaining agility, reliability, and scalability in modern software development practices.</p> <ol> <li>Set up GitHub repository We should start with creating or configuring a GitHub repository for a project. In our scenario we will use <code>main</code> branch that will be protected. Good start for <code>Branch protection rule</code> for <code>main</code> branch should look like this:</li> <li><code>Require a pull request before merging</code> with <code>Require approvals: 1</code> should be turned on. With such configuration we are sure that review process is established for repository</li> <li><code>Require status checks to pass before merging</code> with <code>Require branches to be up to date before merging</code> and CI job we will create in next steps should be added in <code>Status checks that are required</code>. This way we will be sure that our automation is passing on latest main code and there is no way to merge any pull request without it. By default GitHub is not forcing workflows to pass before merging.</li> </ol> <p>There are more options worth considering, but they should be applied according to project's needs.</p> <ol> <li>Create GitHub CI Workflow - code validation All GitHub Actions workflows should be stored in <code>.github/workflows</code> folder, where we can define multiple workflows. For our CI/CD pipeline we can start with such <code>on</code> clause: <pre><code>on:\n  pull_request:\n    branches: [ main ]\n    paths:\n      - \"path_with_code_changes/**\"\n  workflow_dispatch:\n</code></pre> This way workflow will be executed every time any change appears within pull request. There is also possibility to run workflow manually with <code>workflow_dispatch</code>, where we can also add additional properties like <code>debug</code> in case we need more information about failing workflow.</li> </ol> <p>Inside, we can put basic CI checks, depending on technology we are using, below is simple example for parsing dbt: <pre><code>  validate_dbt_project:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4.2.0\n      - uses: actions/setup-python@v5.2.0\n        with:\n          python-version: \"3.12\"\n\n      - name: Install dependencies\n        env:\n          DBT_ENV_SECRET_GITHUB_TOKEN: ${{ secrets.SECRET_GH_TOKEN }}\n          PIP_ROOT_USER_ACTION: ignore\n        run: pip install -qr path_with_code_changes/requirements.txt --no-build-isolation\n\n      - name: Install dbt package depenedencies\n        working-directory: path_with_code_changes\n        env:\n          DBT_ENV_SECRET_GITHUB_TOKEN: ${{ secrets.SECRET_GH_TOKEN }}\n        run: dbt -q deps\n\n      - name: Mount profiles.yml\n        # Required by dbt parse.\n        # The var is the contents of the profiles.yml, encoded in base64:\n        # echo .github/workflows/act/profiles.yml | base64\n        env:\n          PROFILES_YML_BASE64: ${{ vars.PROFILES_YML_BASE64 }}\n        run: echo $PROFILES_YML_BASE64 | base64 --decode &gt; path_with_code_changes/profiles.yml\n\n      - name: Validate the dbt project\n        working-directory: path_with_code_changes\n        env:\n          DBT_ENV_SECRET_GITHUB_TOKEN: ${{ secrets.SECRET_GH_TOKEN }}\n        run: dbt -q parse\n</code></pre></p> <p>Such workflow can be save within <code>ci.yml</code> file, and validate_dbt_project can be added to <code>Status checks that are required</code> explained in previous step. The thing that need to be watched out here is the fact, that <code>ci.yml</code> might not be executed for every change. To enable mandatory check, it's better to run such CI for every pull request.</p> <ol> <li>Docker image build &amp; push GitHub CI workflow</li> </ol> <p>In separate workflow file CD part can be prepared, where basic setup is focused on building and pushing docker image. <code>on</code> clause can be set to be executed only on main branch, or we can have separate job that will build docker image on pull request, with <code>push</code> being enabled only on main. Better approach is to check build process already on pull_request, that's why below pipeline will be used both for <code>pull_request</code> and <code>push</code>: <pre><code>name: Build and Push Docker Image\n\non:\n  pull_request:\n    branches:\n      - main\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout the code\n        uses: actions/checkout@v4.2.0\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3.6.1\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3.3.0\n        with:\n          registry: ghcr.io\n          username: ${{ secrets.GITHUB_ACTOR }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build Docker Image\n        id: docker_build\n        uses: docker/build-push-action@v6.7.0\n        with:\n          context: .\n          file: ./Dockerfile\n          platforms: linux/amd64,linux/arm64\n          tags: ghcr.io/${{ github.repository }}/my-image:tag\n          push: ${{ github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main' }}\n</code></pre> This way on every change in our pull request, docker image will be build only, and once we are comfortable, after merging changed to <code>main</code>, push will happen to GitHub Container Registry (ghcr). In order to use image already on non-prod environment before merging changes, there is an option to modify tagging of the images, where we can use branch name as a tag on pull request, while using proper tagging for pushed changes to main.</p> <ol> <li>Helm Chart Workflow</li> </ol> <p>Similarly to docker images, we can handle lint, package and push process for helm charts. In pull request we can use chart-testing (ct) library that will validate correctness of our yml files, and once we are sure that it's working as expected, workflow executed on main branch should package and push helm chart.</p> <p>In case of reusing Helm chart in multiple projects/repositories, it is good practise to have dedicated repository that will keep helm chart only, and in project repository there should be only helm values file located. It is not a convenient option in highly customized helm charts, where even small change requires upgrading chart. But in most cases only values should be enough to be modified, with bigger changes happening more because of security issues than feature requests.</p> <p>Workflow for Helm Lint, Package and Push, where lint will be executed on both pull request and push, while helm package and push only on main branch workflows. CT (chart testing) is used to validate syntax of helm charts. It can optionally also verify additional things, like incrementing of the helm chart version every time some change was introduced: <pre><code>name: Helm Lint, Package and Push\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'charts/**'\n  pull_request:\n    branches:\n      - main\n    paths:\n      - 'charts/**'\n\njobs:\n  lint-test:\n    name: Lint and Test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4.2.0\n        with:\n          fetch-depth: 0\n\n      - name: Set up Helm\n        uses: azure/setup-helm@v4.2.0\n        with:\n          version: v3.16.1\n\n      - uses: actions/setup-python@v5.2.0\n        with:\n          python-version: '3.12'\n          check-latest: true\n\n      - name: Add necessary dependencies\n        run: |\n            #!/bin/bash\n            helm repo add ${{ vars.CHART_REGISTRY_NAME }} https://{{ vars.CHART_REGISTRY_URL }} --username '${{ secrets.CHART_REGISTRY_USER }}' --password '${{ secrets.CHART_REGISTRY_PASSWORD }}'\n\n      - name: Set up chart-testing\n        uses: helm/chart-testing-action@v2.6.1\n\n      - name: Run chart-testing (lint)\n        run: |\n          ct lint --target-branch ${{ github.event.repository.default_branch }} --check-version-increment=${{ vars.version_increment }}\n</code></pre></p> <p>For helm push we can introduce mechanism that will push only helm charts that are modified. Usage of GitHub action dorny/paths-filter can help with that. It's not mandatory step though, so below code is just assuming that helm package needs to be prepared and pushed. In addition, it is assumed that Chart Museum is used to store all helm charts and GitHub action helm-push-action is used for that:</p> <pre><code>  helm_push:\n      name: \"$Helm Push\"\n      runs-on: ubuntu-latest\n      steps:\n        - name: Checkout\n          uses: actions/checkout@v4\n\n        - uses: dyvenia/helm-push-action@master\n          env:\n            HELM_EXPERIMENTAL_OCI: 1\n            SOURCE_DIR: './charts'\n            CHARTMUSEUM_REPO_NAME: ${{ vars.REPOSITORY_NAME }}\n            CHART_FOLDER: ${{ matrix.directory }}\n            CHARTMUSEUM_URL: '${{ vars.CHART_REGISTRY_URL }}'\n            CHARTMUSEUM_USER: '${{ secrets.CHART_REGISTRY_USER }}'\n            CHARTMUSEUM_PASSWORD: ${{ secrets.CHART_REGISTRY_PASSWORD }}\n</code></pre> <ol> <li>CD preparation</li> </ol> <p>Once there is docker image and helm chart with values prepared, it's good to prepare CD workflow that will deploy application into kubernetes cluster. We are assuming that whole environment is not accessible from the internet in any other way than via VPN, so our GitHub runner should be located on a virtual machine with access to it, ideally within same VPC or connected through VPN to environment. Additionally, such virtual machine should have configured access to Kubernetes cluster through proper RBAC policy or any other option that will limit access to the cluster only to deployments.</p> <p>Once such GitHub runner is configured, it is possible to prepare additional workflow that will handle deployment of the application to DEV and PROD environments. In below example additionally SOPS was configured that is enabling keeping secrets on repository encrypted. To decrypt secrets.yaml files, it is necessary to use eg. AWS KMS, that's why access key and token are provided in last step: <pre><code>name: Deploy\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'etc/helm_values/{{ application_name }}/**'\n  workflow_dispatch:\n\njobs:\n  dev_deploy:\n    runs-on: dev-runner\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4.1.7\n\n    - name: Add necessary dependencies\n      run: |\n        helm repo add ${{ vars.CHART_REGISTRY_NAME }} ${{ vars.CHART_REGISTRY_URL }} --username '${{ vars.CHART_REGISTRY_USER }}' --password '${{ vars.CHART_REGISTRY_PASSWORD }}'\n        helm repo update ${{ vars.CHART_REGISTRY_NAME }}\n\n    - name: Run Helm upgrade commands\n      env:\n        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n      run: |\n        helm secrets upgrade {{ application_name }} --install ${{ vars.CHART_REGISTRY_NAME }}/{{ chart_name }} \\\n          -n {{ namespace }} \\\n          -f etc/helm_values/{{ application_name }}/secrets-dev.yaml \\\n          -f etc/helm_values/{{ application_name }}/values-dev.yaml\n</code></pre></p> <p>Once dev deployment will be successful, there can be manual approval process implemented. Below example requires GitHub Enterprise licence, but there are free alternatives on the market.</p> <pre><code>jobs:\n  prod_deploy_approval:\n    runs-on: ubuntu-latest\n    needs: [ dev_deploy ]\n    environment: prod_approve\n    steps:\n      - name: \"Prod deployment approval\"\n        run: |\n          echo \"Approving deployment to PROD\"\n</code></pre> <p>In addition, environment needs to be prepared in repository settings. <code>Settings &gt; Environments &gt; New environment</code>. After providing environment name (<code>prod_approve</code> in above case), please make sure that <code>Required reviewers</code> checkbox is enabled and there are people/teams provided. Once it is configured, only manual approval from listed people/teams will enable prod deployment. One downside of such approach is the situation when we are not releasing particular version to production. In such situation workflow will stay in <code>Waiting</code> state for 30 days, and after that people from the list will receive notification that this workflow has timed-out.</p> <p>Code for production deployment should look almost identical to dev_deploy job above. The only difference should be with secrets and values files, and also dedicated production GitHub runner should be used.</p> <p>With these configuration we managed to prepare basic CI/CD workflow for our application. Improvements can we introduce: - introduce GitOps approach using ArgoCD or similar tool - extend testing phase during CI process - introduce advanced alerting in case of an issue during deployment. Currently only person involved in the process will receive notification from GitHub - prepare workflow for rollback in case of issues with new version - add more CI checks, like verification of hardcoded secrets in repository (it can be handled with GitHub's Code scanning feature) or additional lint script for aligning with company's coding standard - release job that will show on GitHub recent version prepared - further use of GitHub's Environment feature. By using it, there is possibility to check history of deployments directly from GitHub</p>"},{"location":"data_platform/cicd/#data-flow-automation","title":"Data Flow Automation","text":"<p>TBD</p>"},{"location":"data_platform/data_maturity/","title":"Data platform and data maturity","text":""},{"location":"data_platform/data_maturity/#introduction","title":"Introduction","text":""},{"location":"data_platform/data_maturity/#a-typical-data-maturity-chart-the-what","title":"A typical data maturity chart - the what","text":"<p>Frequently, data maturity is discussed only from a high-level what perspective, which is akin to discussing an illness by only looking at its symptoms. You might see a chart such as this:</p> <p></p> <p>While roughly correct, this provides no insight as to what data maturity actually means in practical terms, and more importantly, how to achieve higher maturity levels.</p> <p>It's important to remember that technology never exists in isolation from culture. Therefore, to utilize it fully, it must be part of business strategy and structure. In fact, according to Conway's Law, it's impossible to implement and utilize tooling without proper organization structure and culture in place.</p> <p>Data maturity is a function of a company's technical and cultural development in the domain of analytics:</p> <p>$$ \\begin{aligned} maturity=f(technology, culture) \\newline {(technology, culture)\\in Analytics} \\end{aligned} $$</p> <p>Another way to visualize this relationship:</p> <p></p> <p>Hopefully, we've managed to illustrate that it's impossible to just buy one's way into data maturity, eg. by purchasing latest &amp; greatest technological solutions. Similarly, it's impossible to just will ones way into it: it's possible to run without shoes, but barely running is not the same as competing in running events. Data maturity is a function of both technology and culture, and reaching a high level of analytical maturity requires strategic investment in both areas.</p>"},{"location":"data_platform/data_maturity/#dyvenia-way-the-how","title":"dyvenia way - the how","text":""},{"location":"data_platform/data_maturity/#technology","title":"Technology","text":""},{"location":"data_platform/data_maturity/#culture","title":"Culture","text":""},{"location":"data_platform/data_maturity/#team-topologies","title":"Team topologies","text":"<p>Below we will look at the organizational structure required for a company to utilize a data platform effectively. We will point out the strengths and weaknesses of each of the main structures.</p> <p>We will discuss models in order. We will start from the ones requiring the least, and move to ones requiring the most data maturity to implement successfully.</p>"},{"location":"data_platform/data_maturity/#centralized","title":"Centralized","text":"<p>The centralized model is the way most enterprises have traditionally structured their analytics. In this model, analytics, including the data platform (its implementation and support, access management, data governance, feature development, maintenance, etc.) is managed by a single, centralized team.</p>"},{"location":"data_platform/data_maturity/#decentralized","title":"Decentralized","text":"<p>In the decentralized model, each team is responsible for its own data platform and processing. This model is often seen in smaller companies or startups, where the data platform is not yet a priority. In an enterprise setting, such model can lead not only to massive waste of resources and duplication of effort, but also to security and governance issues.</p> <p>We still frequently see this model in large companies in which the data platform is not yet a priority. It is typically used unofficially alongside the centralized model, where centralized teams become a bottleneck*, leading to the rise of shadow analytics.</p> <p>*As we will discuss below, centralized data teams and platforms do not inherently result in bottlenecks, but frequently become ones depending on the details of a specific data platform implementation (which includes, as we have learned, organizational structure and culture).</p>"},{"location":"data_platform/data_maturity/#hub-and-spoke","title":"Hub and spoke","text":"<p>The hub and spoke model is a hybrid of the centralized and decentralized models. In this model, a centralized team is responsible for the data platform, but each team has a data engineer, analyst or data scientist assigned. This person is responsible for (or helping with) data processing and analysis within their team.</p> <p>This model is popular within companies with a relatively, bot not fully mature data analytics. The \"spoke\" is still required due to either insufficient development of the data platform, insufficient data literacy within the teams, or both.</p>"},{"location":"data_platform/data_maturity/#data-mesh","title":"Data mesh","text":"<p>This is the newest organization model, proposed only in 2019. It relies on a highly developed data platform and literacy within the company.</p> <p>In this model, each team is responsible for its own data platform instance, and the centralized team is responsible for the data platform's underlying infrastructure and governance. This model requires the platform team to follow something along the lines of Jeff Bezos's famous API mandate, where the platform team is responsible for providing an easy-to-use, well-documented, and reliable interface to their platform. However, in this case, the data platform is actually a \"platform of platforms\": it provides required tooling and golden paths for teams to be able to create their own instances of the data platform.</p>"},{"location":"data_platform/data_maturity/#mindset","title":"Mindset","text":""},{"location":"data_platform/data_maturity/#data-driven","title":"Data-driven","text":"<p>If the company does not make decisions based on data, there will be little interest in high-quality tooling and processes around it. Low quality and manual processes will eventually creep in and the data platform will be underutilized, giving way to \"shadow analytics\".</p>"},{"location":"data_platform/data_maturity/#data-trust","title":"Data trust","text":"<p>It's a bit of a chicken-and-egg problem - if the company does not trust the data, it will not make decisions based on it. If the company does not make decisions based on data, it will not invest in data quality and governance. Frequently, the solution is to build trust in data early on, during the platform's implementation phase.</p> <p>Once the initial success is obtained, trust will follow, as the quality of data from a well-implemented platform will far outperform the quality of data obtained from manual or non-standardized processes.</p>"},{"location":"data_platform/data_maturity/#software-developer-mindset","title":"Software developer mindset","text":""},{"location":"data_platform/data_maturity/#building-things-vs-doing-things","title":"Building things vs doing things","text":"<p>Software developer mindset is required to perform analytics in highly data-mature organizations. It includes adopting proven practices from the software development world: version control, code reviews, testing, and CI/CD. More importantly, however, it requires adopting the mindset of building things, rather than simply doing things. This is a crucial distinction: \"building things\" means that the process is repeatable, scalable, well-documented, and is or can easily be automated. \"Doing things\" means that the process can be manual, error-prone, undocumented, and not scalable.</p> <p>The \"building things\" mindset is reflected in some of the more recent concepts in analytics, such as \"data products\" and \"data contracts\". These are simply ways of allowing the analytics department to work in a way that is more similar to the software development department.</p>"},{"location":"data_platform/data_maturity/#skillset","title":"Skillset","text":"<p>Related to the software developer mindset is the skillset required to implement and utilize a data platform effectively. In a data-mature organizations, all employees (not just the analytics teams) must improve their digital and data competencies.</p>"},{"location":"data_platform/data_maturity/#digital-skillset","title":"Digital skillset","text":"<p>Digital skillset includes familiarity with key concepts such as version control, reproducibility, environments, testing, documentation, and data governance.</p>"},{"location":"data_platform/data_maturity/#data-skillset","title":"Data skillset","text":"<p>Data skillset includes working knowledge of BI tooling and visualization, as well as basic data analytics knowledge, such as familiarity with the basics of data types, data cleaning, data validation, etc.</p>"},{"location":"data_platform/deployments/","title":"Process","text":""},{"location":"data_platform/iac/","title":"Infrastructure as Code (IaC)","text":"<p>Infrastructure as Code (IaC) is a practice that involves managing and provisioning infrastructure through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. This approach enables teams to automate infrastructure deployment and manage it with the same tools and processes used for software development.</p>"},{"location":"data_platform/iac/#terraform-for-cloud-environments","title":"Terraform for Cloud Environments","text":""},{"location":"data_platform/iac/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an open-source IaC tool that allows you to define cloud and on-premises resources in human-readable configuration files that you can version, reuse, and share. It supports multiple providers like AWS, Azure, Google Cloud Platform, and many others.</p>"},{"location":"data_platform/iac/#benefits-of-using-terraform","title":"Benefits of Using Terraform","text":"<p>Consistency: Ensures that all environments are provisioned identically. Scalability: Simplifies the management of large-scale infrastructures. Version Control: Infrastructure configurations can be stored in version control systems. Modularity: Encourages the use of modules for reusable infrastructure components. Collaboration: Teams can work together on infrastructure code just like application code with state of the cluster stored on a shared place like AWS S3 or Azure Blob Storage.</p>"},{"location":"data_platform/iac/#setting-up-a-cicd-pipeline-with-terraform","title":"Setting Up a CI/CD Pipeline with Terraform","text":"<p>Setting up CI/CD pipeline with Terraform is fairly simple process, as long as we are starting new project. It is possible to import existing resources and use them further within Terraform, but it's not covered in below example.</p> <ol> <li>Writing Infrastructure Code<ul> <li>Define resources using HashiCorp Configuration Language (HCL).</li> <li>Organize code into modules for reusability.</li> <li>Store code in a version control system like Git.</li> </ul> </li> <li>Version Control Integration<ul> <li>Use branching strategies (e.g., GitFlow) to manage code changes.</li> <li>Implement pull requests for code reviews.</li> </ul> </li> <li>Continuous Integration<ul> <li>Validation: Use terraform validate to check the syntax.</li> <li>Linting: Use tflint to ensure code quality and adherence to best practices.</li> <li>Security Scanning: Use tools like tfsec to identify potential security issues.</li> </ul> </li> <li>Continuous Deployment<ul> <li>Terraform Plan: Generate an execution plan to preview changes.</li> <li>Approval Process: Implement manual approvals for changes to critical environments.</li> <li>Terraform Apply: Apply the changes to the infrastructure.</li> <li>State Management: Use remote backends (e.g., AWS S3, Azure Blob Storage) for storing the state file securely.</li> </ul> </li> <li>Monitoring and Logging<ul> <li>Integrate logging and monitoring tools to track infrastructure changes and performance.</li> <li>Use services like AWS CloudWatch or Azure Monitor.</li> </ul> </li> </ol>"},{"location":"data_platform/iac/#example-cicd-workflow-with-terraform-and-github-actions","title":"Example CI/CD Workflow with Terraform and GitHub Actions","text":"<p><pre><code>name: Terraform CI/CD Pipeline\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  terraform:\n    name: Terraform Plan\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Code\n        uses: actions/checkout@v3\n\n      - name: Set up Terraform\n        uses: hashicorp/setup-terraform@v3\n\n      - name: Terraform Init\n        run: terraform version\n\n      - name: Terraform Validate\n        run: terraform validate\n\n      - name: Terraform Plan\n        run: terraform plan\n\n  approval:\n    runs-on: ubuntu-latest\n    environment: plan_approve\n    steps:\n      - run: echo \"Terraform plan approved, terraform apply will be executed\"\n\n  terraform_apply:\n    name: Terraform apply\n    needs: [ approval ]\n    runs-on: ubuntu-latest\n    steps:\n      - name: Terraform Apply\n        run: terraform apply\n</code></pre> The setup described above assumes the use of a single environment and a GitHub Enterprise license. However, this process can be easily adapted for multiple environments.</p> <p>For example, in a development environment, you may choose to omit the pull request approval process or assign an internal team for approvals, as the changes are not yet applied to production. Once changes are merged into the main or default branch, the full approval and deployment process will apply accordingly. Environment <code>plan_approve</code> has Enterprise feature enabled with deployment protection rule that is forcing manual review.</p>"},{"location":"data_platform/iac/#terraform-best-practices","title":"Terraform best practices","text":"<ol> <li>Use Remote State Storage: Keep the Terraform state file in a remote backend with locking to prevent conflicts. AWS S3 or Azure Blob Storage are the most common examples.</li> <li>Sensitive Data Management: Do not hard-code secrets; use environment variables or secret management tools. Sops is also good idea which enables keeping encoded secrets in repository. </li> <li>Modular Code: Break infrastructure code into modules for better organization and reuse.</li> <li>Automated Testing: Implement automated tests for infrastructure code using tools like Terratest.</li> </ol>"},{"location":"data_platform/iac/#ansible-for-on-premises-environments","title":"Ansible for On-Premises Environments","text":""},{"location":"data_platform/iac/#what-is-ansible","title":"What is Ansible?","text":"<p>Ansible is an open-source automation tool used for configuration management, application deployment, and task automation. It uses a simple, agentless architecture and relies on standard SSH for communication. While it's not used that frequently for cloud environments, it's still a powerful tool for On-Premise environments.</p>"},{"location":"data_platform/iac/#benefits-of-using-ansible","title":"Benefits of Using Ansible","text":"<p>Agentless Architecture: No need to install agents on managed nodes. Easy to Learn: Uses YAML for playbooks, which is human-readable. Idempotent: Ensures that applying configurations multiple times does not produce unintended changes. Extensible: Supports custom modules and plugins.</p>"},{"location":"data_platform/iac/#setting-up-a-cicd-pipeline-with-ansible","title":"Setting Up a CI/CD Pipeline with Ansible","text":"<p>Setting up a CI/CD pipeline with Ansible involves writing playbooks and roles using YAML syntax to define tasks, organizing these tasks into modular roles for reusability, and defining inventories to specify groups of hosts. The playbooks and related files are stored in a version control system like Git, allowing for effective code management through branching strategies and pull requests.</p> <p>Continuous integration practices are applied by performing syntax checks with <code>ansible-playbook --syntax-check</code>, enforcing code quality and best practices with ansible-lint, and utilizing testing tools like Molecule to validate roles and playbooks locally.</p> <p>For continuous deployment, the pipeline automates the execution of playbooks against the inventory to configure systems, manages sensitive data using variables and Ansible Vault, and enhances deployment efficiency through parallel execution of tasks. This unified approach streamlines configuration management and ensures that on-premises environments are consistently and reliably configured through automated processes.</p>"},{"location":"data_platform/iac/#example-cicd-workflow-with-ansible-and-gitlab-cicd","title":"Example CI/CD Workflow with Ansible and Gitlab CI/CD","text":"<p><pre><code>stages:\n  - test\n  - deploy\n\nvariables:\n  ANSIBLE_HOST_KEY_CHECKING: 'False'\n\nbefore_script:\n  - apt-get update &amp;&amp; apt-get install -y ansible\n\ntest:\n  stage: test\n  script:\n    - ansible-playbook playbook.yml --syntax-check\n    - ansible-lint playbook.yml\n\ndeploy:\n  stage: deploy\n  script:\n    - ansible-playbook -i inventory playbook.yml --vault-password-file vault_pass.txt\n  environment: production\n  when: manual\n</code></pre> In Gitlab CI/CD, managing manual approval is handled slightly different. Basic manual step is possible with usage of <code>when: manual</code>, but simplest way to limit group of people that can approve it can be done by making protection environment and assigning only people capable of doing any changes there.</p>"},{"location":"data_platform/ingestion/","title":"Ingestion Architecture","text":"<p>Here compare 3 paradigms:</p> <ul> <li>Embedded ETL (dlt + orchestrator)</li> <li>SaaS ETL (Fivetran)</li> <li>SaaS + Open Source Connectors (Airbyte)</li> </ul>"},{"location":"data_platform/integrations/","title":"Integrations","text":"<pre><code>flowchart LR\n    %% Nodes\n    subgraph SourceSystems [Source Systems]\n        A[\"CRM Systems\"]\n        B[\"ERP Systems\"]\n        C[\"Marketing Platforms\"]\n        D[\"On Prem Apps\"]\n    end\n\n    subgraph IntegrationLayer [Integration Layer]\n        E[\"API Gateway\"]\n        F[\"Message Broker\"]\n        G[\"Transformation Worker\"]\n        H[\"Persistance Database\"]\n        I[\"Sender Worker\"]\n    end\n\n    subgraph DestinationSystems [Destination Systems]\n        J[\"CRM Systems\"]\n        K[\"ERP Systems\"]\n        L[\"Marketing Platforms\"]\n        M[\"On Prem Apps\"]\n    end\n\n    %% Edge connections between nodes\n    %% From Source Systems to Integration Layer\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; E\n    F --&gt; G\n\n    %% Within Integration Layer\n    E --&gt; F\n    E --&gt; G\n    G --&gt; |Entity Resolution| H\n    H --&gt; I\n\n    %% From Integration Layer to Destination Systems\n    I --&gt; J\n    I --&gt; K\n    I --&gt; L\n    I --&gt; M\n\nstyle E color:#FFFFFF, stroke:#2962FF, fill:#2962FF\nstyle I color:#FFFFFF, stroke:#2962FF, fill:#2962FF\n</code></pre>"},{"location":"data_platform/lifecycle/","title":"Data Engineering Lifecycle","text":"<p>The data engineering lifecycle encompasses the various stages of designing, building, maintaining, and optimizing data pipelines that enable organizations to leverage data effectively. Each stage involves specific processes, tools, and best practices to ensure data flows smoothly, is transformed appropriately, and is accessible to stakeholders. The stages of the data engineering lifecycle are as follows: - Generation - Storage - Ingestion - Transformation - Serving</p> <p>The data engineering lifecycle is supported by several critical undercurrents, including security, data governance, DataOps, data architecture, orchestration, and software engineering practices. While these foundational concepts are integral to the success and scalability of data engineering workflows, they will not be covered in detail in this document. However, they remain crucial to our overall approach and should be considered throughout every stage of the lifecycle.</p> <p></p>"},{"location":"data_platform/lifecycle/#1-data-generation","title":"1. Data Generation","text":"<p>Data generation is the first step in the lifecycle, where raw data is created or collected from various sources. These sources can include user activity on applications, sensor data from IoT devices, logs from systems, and external APIs. The goal of this phase is to collect raw, unprocessed data that will be used in downstream processes.</p>"},{"location":"data_platform/lifecycle/#key-activities","title":"Key Activities:","text":"<ul> <li>Event Logging: Capturing user or system events.</li> <li>IoT and Sensor Data: Collecting data from physical devices.</li> <li>API Requests: Pulling data from external sources.</li> <li>Manual Input: Data entered by users or operators.</li> </ul> <p>Diagram Placeholder: Data Generation Process</p> <p></p> <p></p>"},{"location":"data_platform/lifecycle/#2-data-storage","title":"2. Data Storage","text":"<p>Data storage is the foundational component where all ingested and processed data is stored securely and durably. The choice of storage system depends on the volume, velocity, and type of data. Data can be stored in structured databases, semi-structured storage, or unstructured data lakes.</p>"},{"location":"data_platform/lifecycle/#key-activities_1","title":"Key Activities:","text":"<ul> <li>Choosing Storage Systems: Selecting between data warehouses, lakes, and object storage.</li> <li>Partitioning and Indexing: Organizing data for faster access.</li> <li>Security and Compliance: Ensuring data is encrypted and stored in compliance with regulatory requirements.</li> </ul>"},{"location":"data_platform/lifecycle/#3-data-ingestion","title":"3. Data Ingestion","text":"<p>Data ingestion refers to the process of bringing generated data into your data infrastructure. This can be done in real-time or through batch processes. The goal is to move data from its source into a system where it can be stored and transformed.</p>"},{"location":"data_platform/lifecycle/#key-activities_2","title":"Key Activities:","text":"<ul> <li>Streaming Ingestion: Handling real-time data (e.g., Kafka, Pulsar).</li> <li>Batch Ingestion: Periodically importing data (e.g., using ETL tools).</li> <li>API or File-Based Ingestion: Pulling data from external services or systems via APIs or file transfers.</li> </ul> <p>Diagram Placeholder: Data Ingestion Pipeline</p>"},{"location":"data_platform/lifecycle/#4-data-transformation","title":"4. Data Transformation","text":"<p>Data transformation is where the raw data is cleaned, structured, and enriched to make it suitable for analysis or other uses. This phase involves applying business logic, converting data formats, aggregating values, and removing noise.</p>"},{"location":"data_platform/lifecycle/#key-activities_3","title":"Key Activities:","text":"<ul> <li>Data Cleaning: Handling missing, duplicate, or erroneous data.</li> <li>Data Enrichment: Adding additional information or context to raw data.</li> <li>Aggregations and Calculations: Summing, averaging, or transforming data based on business rules.</li> <li>ETL/ELT: Extracting, transforming, and loading data for consumption.</li> </ul>"},{"location":"data_platform/lifecycle/#5-data-serving","title":"5. Data Serving","text":"<p>Once the data is transformed, it needs to be made available for use in real-time systems or analytical tools. Data serving is about making sure that transformed data is accessible and delivered to users, applications, or other systems in a timely and efficient manner.</p>"},{"location":"data_platform/lifecycle/#key-activities_4","title":"Key Activities:","text":"<ul> <li>API Exposure: Making data available through APIs.</li> <li>Data Warehousing: Storing processed data in data warehouses for efficient querying.</li> <li>Data Lakes: Providing storage for large volumes of raw or semi-structured data.</li> <li>Caching and Indexing: Optimizing data for fast retrieval.</li> </ul>"},{"location":"data_platform/lifecycle/#6-analytics","title":"6. Analytics","text":"<p>After data is stored and made available, it is used for analytics to extract insights. This stage typically involves querying data, building reports, dashboards, and conducting exploratory analysis. The insights derived here help inform business decisions.</p>"},{"location":"data_platform/lifecycle/#key-activities_5","title":"Key Activities:","text":"<ul> <li>BI Tools: Generating reports and dashboards using tools like PowerBI, Tableau, or Looker.</li> <li>Exploratory Data Analysis: Querying data to find patterns, trends, and insights.</li> <li>KPI Tracking: Monitoring key performance indicators through dashboards.</li> </ul>"},{"location":"data_platform/lifecycle/#7-machine-learning","title":"7. Machine Learning","text":"<p>Machine learning is an optional stage in the data engineering lifecycle. Not all data pipelines will require machine learning, but for those that do, this stage is where cleaned and transformed data is used to train, validate, and deploy predictive models. Machine learning models help automate decision-making processes or provide forecasts based on historical data patterns.</p>"},{"location":"data_platform/lifecycle/#key-activities_6","title":"Key Activities:","text":"<ul> <li>Model Training: Using historical data to train predictive models.</li> <li>Feature Engineering: Creating new variables that better represent underlying patterns in the data.</li> <li>Model Deployment: Putting machine learning models into production to make predictions in real time.</li> <li>Monitoring Models: Tracking model performance over time and ensuring they are accurate and reliable.</li> </ul>"},{"location":"data_platform/lifecycle/#8-reverse-etl","title":"8. Reverse ETL","text":"<p>Reverse ETL is the process of moving data from your centralized data warehouse or data lake back into operational systems (such as CRMs, marketing platforms, or custom applications). This stage is about operationalizing data by sending it back into the tools that teams use day-to-day.</p>"},{"location":"data_platform/lifecycle/#key-activities_7","title":"Key Activities:","text":"<ul> <li>Syncing Data: Moving data back into operational systems like Salesforce, HubSpot, or custom tools.</li> <li>Operational Analytics: Making transformed data actionable in real-time for business operations.</li> <li>Data Enrichment: Enhancing third-party tools with enriched or aggregated data from internal systems.</li> </ul> <p>Diagram Placeholder: Reverse ETL Process</p> <p></p>"},{"location":"data_platform/lifecycle/#conclusion","title":"Conclusion","text":"<p>The data engineering lifecycle is a continuous and iterative process that ensures data is captured, processed, and delivered to end-users in a way that enables valuable insights, decision-making, and operational efficiency. From generation to storage, and from analytics to machine learning and reverse ETL, each step plays a critical role in managing the flow of data through an organization.</p>"},{"location":"data_platform/methodology_overview/","title":"Overviews of methodologies used in data-driven organisations","text":"<p>In the rapidly evolving landscape of data-driven decision-making, establishing a robust and efficient data platform is crucial for organizational success. This document presents an overview of the foundational concepts that underpin our data platform methodology. By embracing these principles, we aim to enhance our data engineering practices, streamline operations, and foster innovation. In some areas more technical details are described on a separate page, also with implementation examples.</p> <p>This chapter focuses on the following key areas:</p> <ul> <li>Data Engineering Lifecycle: Understanding the end-to-end process of designing, building, and maintaining data systems to ensure data is reliable, accessible, and valuable.</li> <li>Infrastructure as Code (IaC): Adopting code-based management of infrastructure to achieve consistency, scalability, and efficiency across our technological environments.</li> <li>Continuous Integration and Continuous Deployment (CI/CD): Implementing practices that automate code integration and deployment, enabling faster delivery and higher quality in our data projects.</li> <li>Cloud vs On-Premises Data Platforms: Evaluating the benefits and considerations of cloud-based and on-premises solutions to determine the optimal approach for our data platform needs.</li> <li>GitOps:</li> <li>Data Governance Framework:</li> <li>Agile Practices:</li> <li>DevOps integration:</li> <li>Release versioning</li> <li>SOPS</li> <li>more TBD</li> </ul> <p>By delving into these concepts, we aim to establish a common understanding that will guide our data platform initiatives and support organization's strategic objectives.</p>"},{"location":"data_platform/methodology_overview/#data-engineering-lifecycle","title":"Data Engineering Lifecycle","text":"<p>The Data Engineering Lifecycle encompasses the end-to-end process of designing, building, and maintaining data systems. It ensures that data is collected, processed, stored, and made accessible in a reliable and efficient manner.</p> <p>We can divide the data engineering lifecycle into five stages: [1] - Generation - Storage - Ingestion - Transformation - Serving data</p> <p></p> <p>Beginning of the data engineering lifecycle is the process of getting data from source systems and storing it. Later, data needs to be transformed in a way that will enable serving it to analysts, data scientists, and others. During all stages data needs to be stored somewhere, that's why on diagram storage \"stage\" is showed as a foundation that underpins other stages. Additionally, it's not always continuous flow, as ingestion and transformation can be mixed or repeated, depending on business needs.</p> <p>All of these stages requires some processes that are helping with keeping proper structure for the whole flow, and they are undercurrents that cut across multiple stages of the data engineering lifecycle: security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>For more detailed explanation check Data engineering lifecycle</p>"},{"location":"data_platform/methodology_overview/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>Infrastructure as Code is the practice of managing and provisioning infrastructure through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. Most important concepts are: - Declarative Configurations: Desired state of infrastructure is defined in code, with no manual changes happening anywhere in infrastructure - Version Control: Thanks to storing infrastructure code in repository (eg. Git), all changes are tracked. - Automation Tools: With usage of tools like Terraform, Ansible or CloudFormation, provisioning is automated.</p> <p>For MVP project, manually created infrastructure might be enough. As soon as the project is going into more mature phase, implementing IaC brings a lot of advantages, as the infrastructure is consistent across multiple environments. It's also much easier to scale the environment, as replication of infrastructure to accommodate growth is mostly handled by automation tools. While initial setup is more complex compared to manual implementation, in the long run the whole process is more efficient thanks to reduced manual configuration efforts and associated errors.</p> <p>More information and example implementation can be found on dedicated page</p>"},{"location":"data_platform/methodology_overview/#continuous-integration-and-continuous-deployment-cicd","title":"Continuous Integration and Continuous Deployment (CI/CD)","text":"<p>CI/CD, an integral part of DevOps, brings development and operations teams together by streamlining continuous integration and continuous delivery. It automates most, if not all, of the manual steps traditionally required to push new code from commit to production. This includes phases like building, testing (unit, integration, and regression tests), deployment, and even infrastructure provisioning. By leveraging a well-configured CI/CD pipeline, development teams can seamlessly implement code changes that are automatically tested and deployed, reducing downtime and accelerating release cycles.</p>"},{"location":"data_platform/methodology_overview/#why-is-cicd-important","title":"Why is CI/CD important?","text":"<p>In today's fast-moving tech landscape, CI/CD is no longer just a buzzword\u2014it\u2019s a fundamental pillar of modern software development. By automating everything from code integration to deployment, CI/CD empowers teams to release features and fixes more quickly and frequently, making products more adaptable to user needs. Continuous integration and delivery catch errors early, minimizing downtime and boosting software quality. Additionally, faster feedback loops with stakeholders help ensure the final product meets user expectations. In short, CI/CD is essential for any team striving for rapid, high-quality development.</p>"},{"location":"data_platform/methodology_overview/#what-is-continuous-integration-ci","title":"What is Continuous Integration (CI)","text":"<p>Continuous Integration (CI) is a development practice where developers regularly merge their code changes into a shared repository. Each integration triggers an automated build and testing process, ensuring that code changes are continuously validated. This early and frequent testing helps catch errors or conflicts early, reducing integration issues and improving overall software quality. CI encourages collaboration and streamlines development by making sure that the codebase remains stable and deployable at all times.</p> <p></p>"},{"location":"data_platform/methodology_overview/#what-is-continuous-deployment-and-continuous-delivery","title":"What is Continuous Deployment and Continuous Delivery","text":"<p>Continuous Delivery (CD) is the practice of automatically preparing code changes for release to production. After code passes all automated tests in the CI phase, it's packaged and ready for deployment. However, the final step\u2014pushing the code to production\u2014remains a manual decision. This ensures that the software is always in a deployable state, allowing teams to release updates frequently and reliably whenever they're ready.</p> <p>Continuous Deployment, on the other hand, takes this process a step further by automating the entire release pipeline, including the deployment to production. Every change that passes automated tests is automatically pushed to live environments without human intervention. This enables faster release cycles and ensures that updates and new features are delivered to users as soon as they're ready.</p> <p>For implementing CI/CD in the Data Engineering space, please check out this page, where example of building CI/CD pipeline for application is shown, but also a way to implement such practices into data pipelines.</p>"},{"location":"data_platform/methodology_overview/#cloud-vs-on-premises-data-platforms","title":"Cloud vs On-Premises Data Platforms","text":"<p>TBD</p>"},{"location":"data_platform/orchestration/","title":"Orchestration","text":"<p>Orchestration in the context of data engineering is a critical component that powers complex data workflows across diverse environments and infrastructures. As businesses scale and data processes become more intricate, the need for automated, reliable, and efficient orchestration solutions becomes paramount. This document delves into the essence of orchestration, highlighting its role, importance, and the nuances among the leading tools tailored for Python environments, such as Prefect, Airflow, and Dagster.</p>"},{"location":"data_platform/orchestration/#what-is-an-orchestrator","title":"What is an Orchestrator?","text":"<p>An orchestrator is a software tool designed to automate, manage, and monitor the execution of workflows in a structured and reliable manner. In data engineering, an orchestrator coordinates the tasks that make up data pipelines, ensuring that these tasks are executed in a specified order and within defined constraints. This can include scheduling tasks, handling dependencies between them, and managing their execution across distributed systems.</p>"},{"location":"data_platform/orchestration/#core-functions","title":"Core Functions","text":"<ul> <li>Task Scheduling: Orchestrators manage when each task within a pipeline should run, often using time-based schedules or triggers based on the completion of other tasks.</li> <li>Dependency Management: They keep track of dependencies between tasks, ensuring that tasks that rely on the output of others are executed at the right time and in the right order.</li> <li>Resource Allocation: Orchestrators allocate resources required for executing tasks, which can involve spinning up containers or virtual machines, allocating CPU or memory resources, and more.</li> <li>Error Handling: They manage failures gracefully by retrying tasks, sending alerts, or even failing the entire workflow depending on the criticality of the task.</li> <li>Logging and Monitoring: Orchestrators provide insights into the execution of tasks, offering logs, alerts, and visual representations of workflows to help monitor and debug pipelines.</li> </ul>"},{"location":"data_platform/orchestration/#benefits-in-data-engineering","title":"Benefits in Data Engineering","text":"<p>In data engineering, orchestrators are indispensable for managing data workflows that are too complex or large to handle manually. They help ensure that data flows smoothly through various processes, from extraction and loading to transformation and storage, all while maintaining data integrity and timeliness. This automation not only reduces the risk of errors but also frees up data engineers to focus on higher-value tasks such as data analysis and optimization.</p>"},{"location":"data_platform/orchestration/#comparison-of-orchestration-tools","title":"Comparison of Orchestration Tools","text":"<p>In the Python ecosystem, there are several prominent tools for orchestrating workflows: Prefect, Apache Airflow, and Dagster. While they all share the same basic functionality of managing data workflows, each has its own strengths and focuses, making them better suited for different use cases.</p>"},{"location":"data_platform/orchestration/#prefect","title":"Prefect","text":"<ul> <li> <p>Overview: Prefect is a modern, developer-friendly orchestration tool that emphasizes simplicity and flexibility. It allows users to define workflows using Python code and focuses on reducing the operational burden often associated with running data pipelines.</p> </li> <li> <p>Strengths:</p> </li> <li>Dynamic Workflows: Prefect supports dynamic task generation at runtime, which makes it flexible for workflows where tasks are determined based on data or external inputs.</li> <li>Ease of Use: Prefect\u2019s API is designed to be intuitive and Pythonic, making it accessible to both engineers and data scientists. Workflows are defined as standard Python functions.</li> <li>Hybrid Execution: Prefect can manage workflows that run on different environments, from local development setups to cloud infrastructure, without needing to change the code.</li> <li>Error Handling: Prefect has robust error handling capabilities with features like task retries, timeouts, and automatic recovery mechanisms.</li> <li> <p>Prefect Cloud: Offers a cloud-based monitoring and management platform that simplifies operational tasks, such as workflow monitoring, logging, and alerting.</p> </li> <li> <p>Weaknesses:</p> </li> <li>Dependence on Prefect Cloud (Optional): While Prefect can be run locally, many advanced features (e.g., real-time monitoring, alerts, dashboards) are only available through its paid cloud platform.</li> <li>Younger Ecosystem: Prefect\u2019s community and plugin ecosystem are still growing, which means there might be fewer integrations compared to more mature tools like Airflow.</li> </ul>"},{"location":"data_platform/orchestration/#apache-airflow","title":"Apache Airflow","text":"<ul> <li> <p>Overview: Apache Airflow is a well-established, widely used open-source workflow orchestration tool. It uses Directed Acyclic Graphs (DAGs) to define workflows and has a large ecosystem of plugins and integrations.</p> </li> <li> <p>Strengths:</p> </li> <li>Mature Ecosystem: Airflow has been around since 2014, and its large community means that there are extensive resources, plugins, and integrations available for virtually any use case.</li> <li>Scalability: Airflow is highly scalable, capable of handling large and complex workflows with many dependencies. It is commonly used in production environments across large organizations.</li> <li>Extensive Scheduling Options: Airflow has a powerful scheduler, making it easy to run workflows based on time-based triggers, external events, or dependencies.</li> <li> <p>Wide Adoption: Due to its maturity, Airflow is widely adopted and trusted by many companies for their data orchestration needs. As a result, it's easy to find tutorials, plugins, and community support.</p> </li> <li> <p>Weaknesses:</p> </li> <li>Complexity: Airflow\u2019s declarative DAG syntax can be cumbersome, especially for beginners. Even simple workflows can require significant boilerplate code.</li> <li>Static DAGs: Airflow's DAGs are defined statically, meaning the task structure cannot change dynamically during execution. This can be limiting for more complex or conditional workflows.</li> <li>Operational Overhead: Setting up and maintaining Airflow, particularly in large deployments, can require substantial DevOps effort, especially if not using managed solutions like Astronomer.</li> <li>Concurrency Management: Airflow can struggle with high-concurrency tasks, and it requires careful tuning to handle large-scale workloads efficiently.</li> </ul>"},{"location":"data_platform/orchestration/#dagster","title":"Dagster","text":"<ul> <li> <p>Overview: Dagster is an orchestration tool with a strong focus on data quality, type-safety, and testing. It is designed to make data pipelines more modular, reusable, and easy to maintain, emphasizing workflows that are well-tested and type-checked.</p> </li> <li> <p>Strengths:</p> </li> <li>Data-Aware Orchestration: Dagster treats data and transformations as first-class citizens, which allows for more introspection into data dependencies, types, and flow through the pipeline.</li> <li>Type Checking and Validation: Dagster enforces type checks on inputs and outputs of tasks, which helps ensure data quality and allows developers to catch errors early in the development process.</li> <li>Modularity: Dagster promotes highly modular pipelines by allowing workflows to be broken down into smaller, reusable components (called \"solids\" and \"pipelines\"). This makes it easier to maintain and scale.</li> <li>Development Workflow: It provides tools for local development, including testing utilities and a built-in environment for simulating pipeline runs before deploying them to production.</li> <li> <p>Rich UI: Dagster has a modern UI that provides real-time visibility into task execution, making it easier to monitor and debug workflows.</p> </li> <li> <p>Weaknesses:</p> </li> <li>Steeper Learning Curve: Dagster introduces several new concepts like solids, pipelines, and repositories, which can make it harder for beginners or teams looking for a quick setup.</li> <li>Smaller Community: Dagster has a smaller user base compared to Airflow, so there may be fewer community-driven resources and third-party plugins available.</li> <li>Overhead for Simpler Workflows: For simple workflows, Dagster might feel over-engineered due to its focus on modularity and testing.</li> </ul>"},{"location":"data_platform/orchestration/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature Prefect Apache Airflow Dagster Ease of Setup Easy Complex Moderate Dynamic Workflows Yes No Yes Task Definition Pythonic functions Declarative (DAGs) Modular (Solids/Pipelines) Error Handling Built-in (retries, etc.) Customizable Strong validation UI and Monitoring Prefect Cloud (Optional) Built-in UI Built-in UI Type Checking No No Yes Extensibility Plugins and Integrations Large plugin ecosystem Growing library of plugins Community Support Growing Large and mature Smaller, but active Resource Management Yes (Cloud/Hybrid support) Yes Yes"},{"location":"data_platform/orchestration/#conclusion","title":"Conclusion","text":"<p>Choosing the right orchestrator depends on the specific needs of your team and the complexity of your workflows:</p> <ul> <li>Prefect is ideal for teams looking for an easy-to-use, dynamic orchestration tool with optional cloud monitoring features.</li> <li>Apache Airflow remains a go-to solution for teams that need scalability, a vast plugin ecosystem, and a mature tool with widespread community support.</li> <li>Dagster is a great fit for those who prioritize data quality, modularity, and testing, especially in data-centric environments.</li> </ul>"},{"location":"data_platform/overview/","title":"Overview of Data Platforms","text":"<p>A data platform is an integrated set of technologies that collectively enable the collection, storage, processing, management, and delivery of data throughout an organization. It serves as the foundational infrastructure that supports data engineering, data analytics, machine learning, and data-driven decision-making processes.</p> <p></p>"},{"location":"data_platform/overview/#why-data-platforms-are-essential","title":"Why Data Platforms are Essential","text":"<p>In today's data-driven world, organizations generate and consume vast amounts of data from various sources. A data platform provides a unified and scalable environment to handle this data efficiently, enabling organizations to:</p> <ul> <li>Make Informed Decisions: By providing timely and accurate data, organizations can make evidence-based decisions.</li> <li>Enhance Operational Efficiency: Automate data workflows to reduce manual effort and errors.</li> <li>Foster Innovation: Enable data scientists and analysts to explore data and develop new insights or products.</li> <li>Maintain Data Governance and Compliance: Ensure data security, privacy, and compliance with regulations.</li> </ul>"},{"location":"data_platform/overview/#core-components-of-a-data-platform","title":"Core Components of a Data Platform","text":"<p>A robust data platform typically consists of several key components, each serving a specific purpose in the data lifecycle:</p>"},{"location":"data_platform/overview/#1-data-ingestion","title":"1. Data Ingestion","text":"<p>This layer is responsible for collecting data from various sources, which can include databases, APIs, files, and streaming data.</p> <ul> <li>Batch Ingestion: Periodically transferring large volumes of data.</li> <li>Real-time Ingestion: Capturing data continuously as it is generated.</li> </ul>"},{"location":"data_platform/overview/#2-storage","title":"2. Storage","text":"<p>Stores the ingested data in a reliable and scalable manner.</p> <ul> <li>Data Lakes: Store raw, unprocessed data in its native format.</li> <li>Data Warehouses: Store processed, structured data optimized for querying and analysis.</li> <li>Databases: Traditional relational or NoSQL databases for transactional data.</li> </ul>"},{"location":"data_platform/overview/#3-processing-and-transformation","title":"3. Processing and Transformation","text":"<p>Transforms raw data into a usable format through cleaning, enrichment, aggregation, and formatting.</p> <ul> <li>ETL/ELT Processes: Extract, Transform, Load or Extract, Load, Transform workflows.</li> <li>Stream Processing: Real-time data transformations.</li> </ul>"},{"location":"data_platform/overview/#4-orchestration","title":"4. Orchestration","text":"<p>Manages the scheduling, execution, and monitoring of data workflows.</p> <ul> <li>Workflow Management: Define and manage complex data pipelines.</li> <li>Dependency Handling: Ensure tasks execute in the correct order.</li> <li>Error Handling and Recovery: Robust mechanisms to handle failures.</li> </ul>"},{"location":"data_platform/overview/#5-access-layer","title":"5. Access Layer","text":"<p>Provides mechanisms for data consumption by end-users or applications.</p> <ul> <li>APIs and Services: Enable programmatic access to data.</li> <li>Business Intelligence Tools: Support data visualization and reporting.</li> <li>Data Catalogs: Facilitate data discovery and understanding.</li> </ul>"},{"location":"data_platform/overview/#6-data-governance-and-security-layer","title":"6. Data Governance and Security Layer","text":"<p>Ensures data is secure, compliant, and of high quality.</p> <ul> <li>Access Control: Manage who can access what data.</li> <li>Data Quality Management: Validate and monitor data integrity.</li> <li>Compliance and Auditing: Ensure adherence to regulations like GDPR or HIPAA.</li> </ul>"},{"location":"data_platform/overview/#7-analytics-and-machine-learning-layer","title":"7. Analytics and Machine Learning Layer","text":"<p>Enables advanced data analytics and machine learning capabilities.</p> <ul> <li>Analytics Platforms: Tools for complex data analysis and visualization.</li> <li>Machine Learning Frameworks: Support model development and deployment.</li> <li>Model Serving: Deploy trained models for inference in production systems.</li> </ul>"},{"location":"data_platform/overview/#8-monitoring-and-logging-layer","title":"8. Monitoring and Logging Layer","text":"<p>Provides visibility into the operation of the data platform.</p> <ul> <li>System Monitoring: Track the performance and health of infrastructure.</li> <li>Data Pipeline Monitoring: Observe the flow and processing of data.</li> <li>Logging: Record system and application logs for troubleshooting.</li> </ul>"},{"location":"data_platform/overview/#9-infrastructure-layer","title":"9. Infrastructure Layer","text":"<p>The underlying hardware and software that support all other layers.</p> <ul> <li>Cloud Services: AWS, Azure, GCP offerings for scalable resources.</li> <li>On-Premises Servers: Physical servers managed internally.</li> <li>Containerization and Orchestration: Docker and Kubernetes for deploying applications.</li> </ul>"},{"location":"data_platform/overview/#key-features-of-a-data-platform","title":"Key Features of a Data Platform","text":"<ul> <li>Scalability: Ability to handle increasing amounts of data and users without compromising performance.</li> <li>Flexibility: Support for various data types and processing paradigms (batch and real-time).</li> <li>Reliability: Ensuring data is accurate, consistent, and available when needed.</li> <li>Performance: Optimized for fast data processing and query execution.</li> <li>Security: Robust mechanisms to protect data from unauthorized access and breaches.</li> </ul>"},{"location":"data_platform/overview/#how-data-platforms-enable-data-engineering","title":"How Data Platforms Enable Data Engineering","text":"<p>Data platforms are the backbone of data engineering efforts, providing the tools and infrastructure necessary to build and maintain data pipelines. They enable data engineers to:</p> <ul> <li>Automate Data Workflows: Use orchestration tools to schedule and manage pipelines.</li> <li>Process Data Efficiently: Utilize powerful processing engines for transforming data.</li> <li>Ensure Data Quality: Implement validation and cleansing processes.</li> <li>Collaborate Across Teams: Provide a unified environment for different roles to work together.</li> </ul>"},{"location":"data_platform/overview/#the-role-of-data-platforms-in-the-data-engineering-lifecycle","title":"The Role of Data Platforms in the Data Engineering Lifecycle","text":"<p>A data platform supports every stage of the data engineering lifecycle:</p> <ol> <li>Generation: Captures data from various sources.</li> <li>Ingestion: Brings data into the platform.</li> <li>Storage: Holds data securely and durably.</li> <li>Transformation: Converts data into usable formats.</li> <li>Serving: Makes data accessible to consumers.</li> <li>Analytics: Supports data analysis and visualization.</li> <li>Machine Learning: Facilitates the development and deployment of ML models.</li> <li>Reverse ETL: Sends processed data back to operational systems.</li> </ol>"},{"location":"data_platform/overview/#conclusion","title":"Conclusion","text":"<p>A data platform is more than just a collection of tools; it's an ecosystem that enables organizations to leverage their data assets fully. By integrating various technologies and processes, a data platform facilitates the smooth flow of data from its origin to its ultimate use in decision-making and innovation.</p>"},{"location":"data_platform/roles/","title":"Roles in Data-Driven Organizations","text":"<p>In modern organizations, various roles contribute to the design, development, and operation of data-driven systems. These roles collaborate to ensure that data flows smoothly through its lifecycle, from generation to consumption in analytics, machine learning, and business processes. Data engineers act as a crucial link between upstream data producers and downstream data consumers, enabling a seamless data engineering flow.</p> <p>Diagram Placeholder: Data engineering roles </p> <p>Below is an overview of the key roles and their responsibilities. A single person can take on multiple roles, but it's important to distinguish them within the broader organizational landscape.</p>"},{"location":"data_platform/roles/#1-software-engineers","title":"1. Software Engineers","text":"<p>Software engineers focus on designing, developing, and maintaining applications that often act as data generators or consumers. Their primary responsibility is to build robust, scalable systems that integrate with the organization's data architecture.</p>"},{"location":"data_platform/roles/#key-responsibilities","title":"Key Responsibilities:","text":"<ul> <li>Building data-driven applications that interact with data pipelines.</li> <li>Developing APIs and services for data generation and consumption.</li> <li>Ensuring code quality, scalability, and maintainability.</li> </ul> <p>Key Interactions: Work closely with data engineers to ensure smooth integration between application logic and data pipelines.</p>"},{"location":"data_platform/roles/#2-data-architects","title":"2. Data Architects","text":"<p>Data architects are responsible for designing the overall data architecture of the organization. They define how data is structured, stored, and accessed across the entire data ecosystem, ensuring that systems can scale efficiently while maintaining data quality and governance.</p>"},{"location":"data_platform/roles/#key-responsibilities_1","title":"Key Responsibilities:","text":"<ul> <li>Designing databases, data warehouses, and data lakes.</li> <li>Establishing data governance frameworks.</li> <li>Ensuring the scalability and security of the data architecture.</li> </ul> <p>Key Interactions: Collaborate with data engineers, software engineers, and DevOps teams to implement scalable and efficient data storage and access solutions.</p>"},{"location":"data_platform/roles/#3-devops-and-site-reliability-engineers-sres","title":"3. DevOps and Site Reliability Engineers (SREs)","text":"<p>DevOps engineers and SREs focus on the deployment, monitoring, and maintenance of the infrastructure that supports data pipelines and applications. They ensure that the systems are reliable, secure, and scalable, automating as many processes as possible.</p>"},{"location":"data_platform/roles/#key-responsibilities_2","title":"Key Responsibilities:","text":"<ul> <li>Managing the infrastructure for data processing and storage.</li> <li>Implementing CI/CD pipelines for data and software workflows.</li> <li>Monitoring system performance, uptime, and security.</li> </ul> <p>Key Interactions: Work closely with data engineers, software engineers, and machine learning engineers to maintain the operational health of data systems.</p>"},{"location":"data_platform/roles/#4-data-engineers","title":"4. Data Engineers","text":"<p>Data engineers are responsible for building and maintaining data pipelines that transport data across systems. They focus on automating the ingestion, transformation, and serving of data, ensuring it is reliable and accessible for analysis and decision-making.</p>"},{"location":"data_platform/roles/#key-responsibilities_3","title":"Key Responsibilities:","text":"<ul> <li>Designing and building scalable data pipelines.</li> <li>Automating data ingestion, transformation, and serving processes.</li> <li>Ensuring data quality and pipeline efficiency.</li> </ul> <p>Key Interactions: Collaborate with data analysts, data scientists, software engineers, and DevOps teams to ensure data flows smoothly through the organization.</p>"},{"location":"data_platform/roles/#5-data-analysts","title":"5. Data Analysts","text":"<p>Data analysts work with processed data to generate reports and insights that inform business decisions. They use statistical methods, business intelligence tools, and domain knowledge to create dashboards and reports that present key metrics and trends.</p>"},{"location":"data_platform/roles/#key-responsibilities_4","title":"Key Responsibilities:","text":"<ul> <li>Analyzing and interpreting data to generate actionable insights.</li> <li>Building dashboards and reports to track KPIs.</li> <li>Identifying trends and patterns in historical data.</li> </ul> <p>Key Interactions: Work with data engineers and data scientists to access and analyze the data needed for reporting and insights.</p>"},{"location":"data_platform/roles/#6-data-scientists","title":"6. Data Scientists","text":"<p>Data scientists apply statistical and machine learning techniques to extract insights from data and build models that predict future outcomes or optimize processes. They work with both structured and unstructured data to answer complex business questions.</p>"},{"location":"data_platform/roles/#key-responsibilities_5","title":"Key Responsibilities:","text":"<ul> <li>Building predictive models using statistical and machine learning methods.</li> <li>Conducting exploratory data analysis to uncover hidden patterns.</li> <li>Collaborating with machine learning engineers to deploy models into production.</li> </ul> <p>Key Interactions: Collaborate with data engineers, analysts, and machine learning engineers to ensure access to clean data and the smooth deployment of models.</p>"},{"location":"data_platform/roles/#7-machine-learning-engineers","title":"7. Machine Learning Engineers","text":"<p>Machine learning engineers focus on deploying and maintaining machine learning models in production environments. They ensure that models are properly integrated with data pipelines, are scalable, and can be monitored and updated as needed.</p>"},{"location":"data_platform/roles/#key-responsibilities_6","title":"Key Responsibilities:","text":"<ul> <li>Deploying machine learning models into production.</li> <li>Managing model versioning and monitoring performance.</li> <li>Optimizing models for efficiency and scalability.</li> </ul> <p>Key Interactions: Work closely with data scientists, data engineers, and DevOps to ensure models are scalable, reliable, and performant.</p>"},{"location":"data_platform/roles/#conclusion","title":"Conclusion","text":"<p>Each of these roles plays a crucial part in the overall success of a data-driven organization. From generating and transforming data to analyzing and deploying models, the collaboration between these roles ensures that data flows seamlessly through every stage of the lifecycle, enabling businesses to harness the full power of their data assets.</p>"},{"location":"marts/methodology/","title":"Methodology","text":"<p>Something</p>"},{"location":"marts/methodology/#gates","title":"Gates","text":""},{"location":"marts/methodology/#gate-1-collaboration","title":"Gate 1 Collaboration","text":"<p>The goal of this gate is to align on accesses, roles and responsibilities, and way of collaboration. This is especially important as developing marts is a cross functional task. Typically, the development of a data mart involves collaboration between:</p> <ul> <li>The data professionals building the mart</li> <li>The IT department</li> <li>The business</li> </ul> <p>Setting the rules of the game from the beginning will save a lot of time later on to align on expectations from each stakeholder.</p>"},{"location":"marts/methodology/#it-workshop","title":"IT Workshop","text":"<p>The IT workshop should be</p>"},{"location":"marts/methodology/#metrics-workshop","title":"Metrics Workshop","text":"<p>Typical Agenda is</p>"},{"location":"marts/methodology/#gate-2-marts","title":"Gate 2 Marts","text":""},{"location":"marts/methodology/#gate-3-reports","title":"Gate 3 Reports","text":""},{"location":"marts/methodology/#templates","title":"Templates","text":"<p>To support with ingestions, you can use the following templates</p> Template name Link Ingestion Request link Ingestion Request link"},{"location":"marts/op_models/","title":"Operating Models","text":"<p>TBD</p>"},{"location":"marts/overview/","title":"Executive Overview of Marts Delivery Model","text":"<p>The Marts delivery model is a gated model. The objective of this gated model is to optimize the process of ingesting, modelling and validating data for reporting consumptions.</p>"},{"location":"organization/op_model/","title":"Process","text":""},{"location":"organization/roles/","title":"Roles","text":""},{"location":"organization/roles/#data-engineer","title":"Data Engineer","text":""},{"location":"organization/roles/#job-description","title":"Job Description","text":""}]}